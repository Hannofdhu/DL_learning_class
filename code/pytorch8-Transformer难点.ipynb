{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4bc3aa",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969b7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 6, 0, 0, 0],\n",
      "        [1, 4, 7, 4, 0]])\n",
      "torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "batch_size = 2\n",
    "\n",
    "#确定词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "#确定特征大小\n",
    "model_dim = 8\n",
    "#确定序列最大长度\n",
    "max_src_len = 5\n",
    "max_tgt_len = 5\n",
    "\n",
    "#确定序列长度\n",
    "src_len = torch.Tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([3,5]).to(torch.int32)\n",
    "\n",
    "#确定序列，并padding\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max_src_len-L)),0)\\\n",
    "            for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_tgt_words,(L,)),(0,max_tgt_len-L)),0)\\\n",
    "            for L in tgt_len])\n",
    "\n",
    "#构造embedding  torch.nn.embedding,第0行weight要让给padding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1,\\\n",
    "                                   model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1,\\\n",
    "                                  model_dim)\n",
    "#调用Embedding class的forward方法（实例后加括号）\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "print(src_seq)\n",
    "print(src_embedding.shape) #三维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd931675",
   "metadata": {},
   "source": [
    "# Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1007c79",
   "metadata": {},
   "source": [
    "相当于一个大小为（max_position_len,model_dim）的矩阵,与每个句子的word_embedding相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028bfeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "max_position_len = 5\n",
    "\n",
    "#表示括号里的东西\n",
    " #reshape(-1,1)这里-1是指未设定行数，程序随机分配\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1,1)#（5，1）\n",
    "i_mat = torch.pow(10000,torch.arange(0,\\\n",
    "        model_dim,2).reshape(1,-1)/model_dim).reshape(1,-1) #（1,4）\n",
    "#计算PE\n",
    "\n",
    "pe_embedding_table = torch.zeros((max_position_len,\n",
    "                                  model_dim))\n",
    "#偶数行\n",
    "    #向量相除用到了广播机制\n",
    "pe_embedding_table[:,0::2] = torch.sin(pos_mat/i_mat)\n",
    "#奇数行\n",
    "pe_embedding_table[:,1::2] = torch.cos(pos_mat/i_mat)\n",
    "\n",
    "#实例化pe_embedding\n",
    "pe_embedding = nn.Embedding(max_position_len,model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table,requires_grad=False)\n",
    "\n",
    "#提取原始序列的position信息\n",
    "#_只起到遍历作用，传进去的还是max(src_len)\n",
    "# src_pos =torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for _ in src_len]).to(torch.int32)\n",
    "# tgt_pos =torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)),0) for _ in tgt_len]).to(torch.int32)\n",
    "src_pos = torch.Tensor([[i for i in range(max_position_len)],[i for i in range(max_position_len)]]).to(torch.int32)\n",
    "tgt_pos = torch.Tensor([[i for i in range(max_position_len)],[i for i in range(max_position_len)]]).to(torch.int32)\n",
    "src_seq_embedding = pe_embedding(src_pos)\n",
    "tgt_seq_embedding = pe_embedding(tgt_pos)\n",
    "print(src_seq_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08262eec",
   "metadata": {},
   "source": [
    "# Encoder Self-Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342b1e4",
   "metadata": {},
   "source": [
    "原文中称为Scaled Dot-Product Attention:\n",
    "\n",
    "$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "对于一批句子，如果Q大小为（2，5，8），Kt大小为（2，8，5），则它们想乘是一个（2，5，5）的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c1447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1821, 0.1993, 0.2137, 0.2179, 0.1869]) tensor([1.4296e-08, 1.2058e-04, 1.2810e-01, 8.7178e-01, 1.9469e-07])\n"
     ]
    }
   ],
   "source": [
    "#scale的重要性\n",
    "\n",
    "#概率层面\n",
    "alpha1,alpha2 = 0.1,10\n",
    "score = torch.randn(5)\n",
    "prob1 = F.softmax(alpha1*score,-1)#-1代表维度\n",
    "prob2 = F.softmax(alpha2*score,-1)\n",
    "print(prob1,prob2)\n",
    "#score乘以较大的数导致概率差别会很大（e-01代表0.1）\n",
    "\n",
    "#雅可比层面（alpha2*score可能使梯度变得接近零，导致很难训练）\n",
    "##记得补上##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca64dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.6170, -1.5600,  1.6287, -0.1512, -1.1252],\n",
      "         [ 0.2792, -0.8092,  0.5152,  1.1749,  0.0832],\n",
      "         [ 1.5149, -2.0074, -0.4100, -0.4935, -0.1545],\n",
      "         [-0.3908, -0.7358, -0.2171,  0.3309,  0.3768],\n",
      "         [-1.3045, -1.0203, -2.2219,  0.1250, -0.1415]],\n",
      "\n",
      "        [[ 0.3674,  0.1926, -0.7320, -1.7360, -0.1688],\n",
      "         [ 1.0316,  1.3850, -0.2825, -1.1772, -0.6705],\n",
      "         [-1.2979,  0.9367, -0.6398, -0.3711, -0.3190],\n",
      "         [-0.7012,  1.7016, -0.9971, -2.0819,  1.9150],\n",
      "         [-0.5901,  0.6386,  1.7757,  1.4461,  0.6530]]])\n",
      "tensor([[[-2.6170, -1.5600,    -inf,    -inf,    -inf],\n",
      "         [ 0.2792, -0.8092,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[ 0.3674,  0.1926, -0.7320, -1.7360,    -inf],\n",
      "         [ 1.0316,  1.3850, -0.2825, -1.1772,    -inf],\n",
      "         [-1.2979,  0.9367, -0.6398, -0.3711,    -inf],\n",
      "         [-0.7012,  1.7016, -0.9971, -2.0819,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf]]])\n",
      "tensor([[[0.2579, 0.7421, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7481, 0.2519, 0.0000, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[0.4358, 0.3659, 0.1451, 0.0532, 0.0000],\n",
      "         [0.3568, 0.5081, 0.0959, 0.0392, 0.0000],\n",
      "         [0.0676, 0.6313, 0.1305, 0.1707, 0.0000],\n",
      "         [0.0766, 0.8471, 0.0570, 0.0193, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan]]])\n"
     ]
    }
   ],
   "source": [
    "#构造Encoder Self-Attention Mask\n",
    "#mask的shape,[batch_size,max(src_len),mac(src_len)],值为-或-inf\n",
    "#src有效的position,padding都被记为0\n",
    "valid_encoder_pos =torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max_src_len-L)),0) for L in src_len])\n",
    "#再次扩维\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2)\n",
    "#bmm代表batch matrix multiply，得到有效邻接矩阵\n",
    "valid_encoder_adjacency = torch.bmm(valid_encoder_pos\\\n",
    "                                   ,valid_encoder_pos.transpose(1,2))\n",
    "#无效矩阵\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_adjacency\n",
    "#编码器自注意力mask矩阵\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "\n",
    "#初始化score\n",
    "#(2,5,5)\n",
    "score = torch.randn(batch_size,max_src_len,max_src_len)\n",
    "print(score)\n",
    "#padding部分不参与自注意力计算\n",
    "#mask填充，需要一个bool类型的矩阵mask_encoder_self_attention\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention,-np.inf)\n",
    "print(masked_score)\n",
    "#进入mask得到注意力权重\n",
    "prob = F.softmax(masked_score,-1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e6ea5",
   "metadata": {},
   "source": [
    "# intra attention mask(cross Multi head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3137498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 1])\n",
      "torch.Size([2, 5, 1])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "tensor([[[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[[False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "#计算Q*K^T shape(batch_size,tgt_seq_len,src_seq_len)\n",
    "#Q shape(h,tgt_seq_len) K shape (h,src_seq_len)\n",
    "\n",
    "#弄出valid_encoder_pos和valid_decoder_pos\n",
    "valid_encoder_pos =torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max_src_len-L)),0) for L in src_len])\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2)\n",
    "\n",
    "valid_decoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max_tgt_len-L)),0) for L in tgt_len])\n",
    "valid_decoder_pos = torch.unsqueeze(valid_decoder_pos,2)\n",
    "\n",
    "valid_cross_pos_matrix = torch.bmm(valid_encoder_pos,valid_decoder_pos.transpose(1,2))\n",
    "invalid_cross_pos_matrix = 1-valid_cross_pos_matrix\n",
    "mask_cross_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "print(valid_encoder_pos.shape)\n",
    "print(valid_decoder_pos.shape)\n",
    "\n",
    "print(valid_encoder_pos)\n",
    "print(valid_decoder_pos)\n",
    "print(valid_cross_pos_matrix)#仅同时没有padding时，才会为1\n",
    "\n",
    "print(mask_cross_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e91834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8148, 0.1852, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8557, 0.1443, 0.0000, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[0.1789, 0.2806, 0.0365, 0.5039, 0.0000],\n",
      "         [0.1115, 0.6399, 0.0176, 0.2310, 0.0000],\n",
      "         [0.1699, 0.2490, 0.3115, 0.2696, 0.0000],\n",
      "         [0.0579, 0.1674, 0.0592, 0.7155, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan]]])\n"
     ]
    }
   ],
   "source": [
    "score = torch.randn(batch_size,max_tgt_len,max_src_len)\n",
    "masked_score = score.masked_fill(mask_cross_attention,-np.inf)\n",
    "prob = F.softmax(masked_score,-1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630df02",
   "metadata": {},
   "source": [
    "# Decoder self attention mask(自回归，强调因果律) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39db5697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1888, 0.8112, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1200, 0.1960, 0.6841, 0.0000, 0.0000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8265, 0.1735, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2251, 0.2024, 0.5725, 0.0000, 0.0000],\n",
      "         [0.1525, 0.2636, 0.2060, 0.3778, 0.0000],\n",
      "         [0.1115, 0.6125, 0.1186, 0.0419, 0.1155]]])\n"
     ]
    }
   ],
   "source": [
    "#第一个是特殊字符\n",
    "\n",
    "#在矩阵左右上下做padd，再unsqueeze,再cat\n",
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(0,max_tgt_len-L,0,max_tgt_len-L)),0)\n",
    "              for L in tgt_len])\n",
    "\n",
    "print(valid_decoder_tri_matrix.shape)\n",
    "print(valid_decoder_tri_matrix)\n",
    "\n",
    "invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "\n",
    "score = torch.randn(batch_size,max_tgt_len,max_tgt_len)\n",
    "\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix,-1e9)\n",
    "\n",
    "prob = F.softmax(masked_score,-1)\n",
    "\n",
    "print(prob)\n",
    "\n",
    "#构建scaled self-attention\n",
    "def scaled_dot_product_attention(Q,K,V,attn_mask):\n",
    "    score = torch.bmm(Q,K.transpose(-2,-1))/torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attn_mask,-1e9)\n",
    "    prob = F.softmax(masked_score,-1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee757fc",
   "metadata": {},
   "source": [
    "# loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1d248a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4508, 1.7205, 0.0000],\n",
      "        [1.0015, 3.0528, 1.2822]])\n",
      "tensor([[1.4508, 1.7205, 0.0000],\n",
      "        [1.0015, 3.0528, 1.2822]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#batchszie,seq_len,vocab_size\n",
    "logits = torch.randn(2,3,4)\n",
    "label = torch.randint(0,4,(2,3))\n",
    "#交叉熵loss\n",
    "logits = logits.transpose(1,2)\n",
    "mean_loss = F.cross_entropy(logits,label)#六个单词平均交叉熵\n",
    "loss = F.cross_entropy(logits,label,reduction='none')\n",
    "\n",
    "\n",
    "#引入mask\n",
    "max_tag_len = 3\n",
    "tgt_len = torch.Tensor([2,3]).to(torch.int32)\n",
    "mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max_tag_len-L)),0) for L in tgt_len])\n",
    "#loss mask\n",
    "#element_wise_multiply\n",
    "print(loss*mask)\n",
    "\n",
    "#设置label为-100也能达到同样的loss mask效果，因为F.cross_entropy的参数ignore_index的存在\n",
    "label[0,2] = -100\n",
    "loss2 =  F.cross_entropy(logits,label,reduction='none')\n",
    "print(loss2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
